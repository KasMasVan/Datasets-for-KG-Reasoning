# source
[The LAMBADA dataset: Word prediction requiring a broad discourse context](https://arxiv.org/pdf/1606.06031.pdf)
# description
>The LAMBADA dataset (Paperno et al., 2016) tests the
ability of systems to model long-range dependencies in
text. The task is to predict the final word of sentences
which require at least 50 tokens of context for a human to
successfully predict.

>LAMBADA
is a collection of narrative passages sharing the characteristic that human subjects
are able to guess their last word if they
are exposed to the whole passage, but not
if they only see the last sentence preceding the target word
# statistics
>The LAMBADA dataset consists of 10,022 passages, divided into 4,869 development and 5,153
test passages (extracted from 1,331 and 1,332 disjoint novels, respectively). The average passage
consists of 4.6 sentences in the context plus 1 target sentence, for a total length of 75.4 tokens (dev)
/ 75 tokens (test). Examples of passages in the
dataset are given in Figure 1.

Please refer to section 3.2 of the 'source' paper for more detail.
# example
![image](https://user-images.githubusercontent.com/51369075/97125037-7a8f0700-176d-11eb-90f3-a2692c32628c.png)

# referenced by
[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
