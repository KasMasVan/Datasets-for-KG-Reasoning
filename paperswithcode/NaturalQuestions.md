# source
[Natural Questions: a Benchmark for Question Answering Research](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf)
# description
>Questions consist of real anonymized, aggregated queries
issued to the Google search engine. An annotator is presented with a question along
with a Wikipedia page from the top 5 search
results, and annotates a long answer (typically a paragraph) and a short answer (one
or more entities) if present on the page,
or marks null if no long/short answer is
present.
# statistics
>The public release consists of
307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples 5-way annotated sequestered as test data
# example

# referenced by
